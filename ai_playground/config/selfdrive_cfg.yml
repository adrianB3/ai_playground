utils:
  neptune_project: adrianb3/ai-playground # neptune.ai project name
  experiments_dir: C:\Users\UIB36644\ai_playground_experiments # where the experiments data should be stored
  load_exp:  #D:\repos\ai_playground_experiments\exp3\exp3_15-06-2020_22_12_14\  # none is you don't want to load any exp
environment:
  env_path: #C:\Users\UIB36644\ai_playground_build\ai_playground.exe
  time_scale: 1.0 # env time acceleration as multiplication factor over real-time
  behavior_name_high_lvl: abstract_brain?team=0
  behavior_name_low_lvl: primitive_brain?team=0
  behavior_name: CarBrain?team=0
icm_params:
  learning_rate: 0.0001
  epsilon: 0.000001
  batch_size: 32
  num_epochs: 10
  scaling_factor: 2
algorithm:
  name: ppo
  params:
    ppo:
      optimizer: adam
      optimizer_params:
        adam:
          learning_rate: 0.00001
          epsilon: 0.000001
      ppo_actor_net: actor_preproc
      ppo_value_net: value_preproc
      num_epochs: 5
      discount_factor: 0.999
      gradient_clipping: 0.7
      entropy_regularization: 0.01
      importance_ratio_clipping: 0.7
      use_gae: True
      use_td_lambda_return: True
      normalize_observations: True
      normalize_rewards: True

    haar:
      num_eps: 1000 # N - number of episodes
      k_s: 10 # min number of steps after which a high level action is required
      k_0: 200 # number of low level steps
      num_skills: 2
      j: 3 # number of ep for high level training
      discount_factor_high_lvl: 0.995
      discount_factor_low_lvl: 0.995
      policy_optimizer: ppo
  networks_params:
    actor_fc_layers: (128, 75)
    value_fc_layers: (75, 40)
train_session:
  num_env_steps: 300000
  run_parallel: False
  num_parallel_envs: 4
  collect_eps_per_iter: 1
  replay_buff_capacity: 100
  num_eval_episodes: 2
  eval_interval: 200 # nb of steps between eval sessions
  summary_interval: 10
  use_tf_function: True
  summaries_flush_secs: 1
  log_interval: 10
  train_checkpoint_interval: 100
  policy_checkpoint_interval: 100